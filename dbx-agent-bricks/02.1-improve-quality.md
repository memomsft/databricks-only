# 04 ‚Äì Improve Quality & Evaluation

Once your Knowledge Assistant agent is created and you‚Äôve tested some prompts, the next step is to **systematically improve quality**.  
Databricks Agent Bricks provides an *Improve Quality* module that lets you build a labeled dataset for evaluation and continuous refinement.

---

## Why Improve Quality?

- **Reliability:** Ensure the agent consistently provides correct answers.  
- **Transparency:** Track how the agent performed against a ‚Äúgolden dataset.‚Äù  
- **Collaboration:** SMEs can review answers through labeling sessions.  
- **Lifecycle:** Labeled examples integrate directly with MLflow for experiment tracking.  

---

## 1. Add a Question Manually

1. Navigate to your Agent ‚Üí **Improve Quality** tab.  
2. Click **Add**.  
3. Enter:  
   - **Question:** natural language query.  
   - **Expected answer:** text snippet of the correct response.  
   - **Label:** mark as `correct`, `incorrect`, or `partial`.  

üëâ Example:  

- **Question:** *‚ÄúWhat is the return policy for defective items?‚Äù*  
- **Expected:** *‚ÄúDefective items may be returned within 30 days with receipt.‚Äù*  
- **Label:** `correct`  

---

## 2. Feedback & Tracking

Once questions are added, the UI displays new columns:

- **ID:** Unique identifier for the example.  
- **Guidelines:** Optional notes (e.g., ‚Äúuse only the latest policy‚Äù).  
- **Feedback records:** Logs the agent‚Äôs responses vs. the expected label.  

This lets you quickly see where the agent is performing well or struggling.

---

## 3. Labeling Sessions (Optional)

On the right panel, you can start a **labeling session**:  

- Send questions to experts/reviewers.  
- They validate whether answers match expectations.  
- Build a **collaborative evaluation dataset** for higher quality.  

This is especially valuable for regulated domains (finance, healthcare, HR, etc.).

---

## 4. Other Options in *Improve Quality*

Besides manual add, you can also:  

- **Import**: Upload a labeled dataset (CSV/JSON).  
- **Export**: Download existing labeled examples to share or reuse.  
- **Expert Feedback:** Route batches of prompts to SMEs for validation.

---

## 5. MLflow Integration

Every Knowledge Assistant agent automatically creates an **MLflow experiment**.  
- Labeled examples flow into this experiment for **evaluation metrics**.  
- You can compare different versions of your agent, prompts, or data sources.  
- This creates a **closed loop**: Test ‚Üí Label ‚Üí Evaluate ‚Üí Improve.  

---

‚úÖ At this point, you have:  
- An initial Knowledge Assistant agent.  
- A **labeled dataset** to track quality.  
- The foundation for **continuous improvement** using MLflow.

